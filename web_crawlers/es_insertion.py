import hashlib
import json
import os
import time
from datetime import date, datetime
from stat import S_ISREG, ST_CTIME, ST_MODE

import boto3
from elasticsearch import Elasticsearch, RequestsHttpConnection
from requests_aws4auth import AWS4Auth
from back.common.es_connection import get_results_from_elastic_search
from article_items_extractor import write_json_files

# By default we connect to localhost:9200
# es = Elasticsearch()

host = 'vpc-yggdrasil-security-oxe3dexfe7xeea5n64kbd5zck4.eu-west-2.es.amazonaws.com'
region = 'eu-west-2'
service = 'es'

es = Elasticsearch(
    hosts=[{'host': host, 'port': 443}],
    use_ssl=True,
    verify_certs=True,
    connection_class=RequestsHttpConnection
)

write_json_files()

dir_path = 'web_crawler/article_files/'

today = datetime.now().date()

for f in os.listdir(dir_path):
    absolute_path = dir_path + f
    filetime = datetime.fromtimestamp(os.path.getctime(absolute_path))

    # article created & scraped TODAY
    if filetime.date() == today:
        print('TRUE ' + str(f))

        with open(absolute_path) as json_file:
            data = json.load(json_file)
            url_data = data['url']
            title_data = data['title']

            query_body = {
                "query": {
                    "bool": {
                        "filter": [
                            {
                                "bool": {
                                    "must": [
                                        {
                                            "term": {
                                                "url.keyword": {
                                                    "value": url_data,
                                                    "boost": 1
                                                }
                                            }
                                        }
                                    ],
                                    "adjust_pure_negative": True,
                                    "boost": 1
                                }
                            }
                        ],
                        "adjust_pure_negative": True,
                        "boost": 1
                    }
                }
            }

            # dummy separation of articles (relevant/irelevant)

            if len(url_data) % 2:
                result = get_results_from_elastic_search(
                    es,
                    "relevant_articles_index",
                    query_body,
                    "hits.hits"
                )
                # check if article already inserted in relevant
                if result == []:
                    print("no match in relevant -> now inserted")
                    res = es.index(
                        index='relevant_articles_index', body=data)
                else:
                    print('already inserted in relevant')
            else:
                result = get_results_from_elastic_search(
                    es,
                    "irelevant_articles_index",
                    query_body,
                    "hits"
                )

                # check if article already inserted in irelevant
                if result['hits'] == []:
                    print("no match in irelevant -> now inserted")
                    res = es.index(
                        index='irelevant_articles_index', body=data)
                else:
                    print('already inserted in irelevant')
