{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"XpaCw0zusyeP","executionInfo":{"status":"ok","timestamp":1655385373908,"user_tz":-180,"elapsed":942,"user":{"displayName":"Vlad Vitan","userId":"08529414929886631328"}}},"outputs":[],"source":["# !kill -9 -1"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655385374502,"user":{"displayName":"Vlad Vitan","userId":"08529414929886631328"},"user_tz":-180},"id":"ndjxnHMhs4M0","outputId":"d178a2c8-bad0-4c39-d22e-814cb0344bac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Jun 16 13:16:13 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","GPU 0: Tesla T4 (UUID: GPU-546cb1b7-9aa0-c241-39fa-f197d4fcaf0a)\n"]}],"source":["!nvidia-smi\n","!nvidia-smi -L"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"duFkfSQjAc8u","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d367012b-fdf0-4257-a6a0-d189db7d8489"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 4.2 MB 37.5 MB/s \n","\u001b[K     |████████████████████████████████| 1.8 MB 67.7 MB/s \n","\u001b[K     |████████████████████████████████| 6.6 MB 74.4 MB/s \n","\u001b[K     |████████████████████████████████| 86 kB 5.6 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 79.8 MB/s \n","\u001b[K     |████████████████████████████████| 181 kB 70.4 MB/s \n","\u001b[K     |████████████████████████████████| 145 kB 73.5 MB/s \n","\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n","\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 4.9 MB 39.5 MB/s \n"]}],"source":["!pip install -q pandas\n","!pip install -q pyyaml h5py\n","!pip install -q transformers wandb\n","!pip install -q -U tensorflow-text==2.8.*\n","!pip install -q tf-models-official==2.7.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLvIkDWT1t2J"},"outputs":[],"source":["import os\n","import json\n","import random\n","import numpy as np\n","import pandas as pd\n","\n","from datetime import datetime\n","from pprint import pprint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sFzmyYG419Pz"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","DRIVE = \"/content/drive/MyDrive/ColabData/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDAKyCvv2E4m"},"outputs":[],"source":["import wandb\n","from wandb.keras import WandbCallback\n","\n","wandb.login()\n","WANDB_PROJECT_NAME = \"longformer_cybsersecurity_news_filter_keras_2\"\n","\n","wandb.init(project=WANDB_PROJECT_NAME, resume=True, config={\"hyper\": \"parameter\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6nzVX4LA2j3D"},"outputs":[],"source":["import tensorflow as tf\n","\n","physical_devices = tf.config.list_physical_devices('GPU')\n","print(\"Num GPUs Available: \", len(physical_devices))\n","tf.config.experimental.set_memory_growth(physical_devices[0], True)\n","\n","from transformers import AutoConfig\n","from transformers import AutoTokenizer\n","from transformers import TFAutoModel\n","from official.nlp import optimization  # to create AdamW optimizer\n","\n","print('TF version',tf.__version__)\n","tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjAAmKpo2_vA"},"outputs":[],"source":["MODEL_MAX_LENGTH=4096\n","\n","tokenizer_path = \"allenai/longformer-base-4096\"\n","tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n","\n","model_path = \"allenai/longformer-base-4096\"\n","\n","config = AutoConfig.from_pretrained(model_path)\n","config.num_labels = 2\n","config.id2label = {0: \"irrelevant\", 1: \"relevant\"}\n","config.attention_window = 128\n","\n","longformer = TFAutoModel.from_pretrained(model_path, config=config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"99p3eq0_yXYD"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def plot_validation_curves(history):\n","    history_dict = history.history\n","    print(history_dict.keys())\n","    \n","    acc = history_dict['binary_accuracy']\n","    val_acc = history_dict['val_binary_accuracy']\n","    loss = history_dict['loss']\n","    val_loss = history_dict['val_loss']\n","    \n","    epochs = range(1, len(acc) + 1)\n","    fig = plt.figure(figsize=(10, 6))\n","    fig.tight_layout()\n","    \n","    plt.subplot(2, 1, 1)\n","    # r is for \"solid red line\"\n","    plt.plot(epochs, loss, 'r', label='Training loss')\n","    # b is for \"solid blue line\"\n","    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","    plt.title('Training and validation loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    \n","    plt.subplot(2, 1, 2)\n","    plt.plot(epochs, acc, 'r', label='Training acc')\n","    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","    plt.title('Training and validation accuracy')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend(loc='lower right')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ehr9MY132EtY"},"outputs":[],"source":["dataset_path = os.path.join(DRIVE, \"text_classification_dataset_2.pickle\")\n","ds = pd.read_pickle(dataset_path)\n","\n","ds.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W8pq1Xbgj_DV"},"outputs":[],"source":["texts = ds[\"text\"].to_list()\n","labels = ds[\"label\"].to_list()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C5oBNAhj5cne"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","def split(texts, labels):\n","    train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, stratify=labels, test_size=0.20, random_state=42)\n","    train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, stratify=train_labels, test_size=0.25, random_state=42)\n","\n","    return train_texts, train_labels, val_texts, val_labels, test_texts, test_labels\n","  \n","train_texts, train_labels, val_texts, val_labels, test_texts, test_labels = split(texts, labels) # 20% test, 20% validation, 60% train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yvf3FvTh5W2Z"},"outputs":[],"source":["relevant, irrelevant = np.bincount(train_labels)\n","total = relevant + irrelevant\n","\n","weight_for_0 = (1 / irrelevant) * (total) / 2.0 \n","weight_for_1 = (1 / relevant) * (total) / 2.0\n","\n","class_weight = {0: weight_for_0, 1: weight_for_1}\n","\n","print('Weight for irrelevant samples (0): {:.2f}'.format(weight_for_0))\n","print('Weight for relevant samples (1): {:.2f}'.format(weight_for_1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jeutcb6n3RJH"},"outputs":[],"source":["def tokenize(texts):\n","    tokenized_texts = tokenizer(texts, truncation=True, padding=True, return_tensors=\"np\")\n","    return tokenized_texts\n","\n","def format_input(texts, labels):\n","    inputs = tokenize(texts).data\n","    labels = np.asarray(labels).astype('float16').reshape((-1,1))\n","\n","    return inputs, labels\n","\n","train_inputs, train_labels = format_input(train_texts, train_labels)\n","val_inputs, val_labels = format_input(val_texts, val_labels)\n","test_inputs, test_labels = format_input(test_texts, test_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o9v3qqElSPL4"},"outputs":[],"source":["print(train_inputs['input_ids'].shape)\n","print(train_inputs['attention_mask'].shape)"]},{"cell_type":"code","source":["train_tokens = train_inputs['input_ids']\n","train_attention = train_inputs['attention_mask']\n","\n","val_tokens = val_inputs['input_ids']\n","val_attention = val_inputs['attention_mask']"],"metadata":{"id":"zLRCTX_nFBXe"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GskNzB1Wv6qo"},"outputs":[],"source":["lf_output_check = longformer.longformer(tokenize(train_texts[0])) # The answer is garbage, use it only to check shapes.\n","\n","print(type(lf_output_check))\n","print(lf_output_check.keys())\n","\n","print(f'Pooler Outputs Shape:{lf_output_check[\"pooler_output\"].shape}')\n","print(f'Pooler Outputs Values:{lf_output_check[\"pooler_output\"][0, :5]}')\n","print(f'Last Hidden State Shape:{lf_output_check[\"last_hidden_state\"].shape}')\n","print(f'Last Hidden State Values:{lf_output_check[\"last_hidden_state\"][0, :5]}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0XM1FN_kGg7b"},"outputs":[],"source":["# Experiment 4: use pooler output layer\n","\n","def create_pooler_model():\n","    input_ids = tf.keras.layers.Input((MODEL_MAX_LENGTH,), name=\"input_ids\", dtype=tf.int32)\n","    attention_mask = tf.keras.layers.Input((MODEL_MAX_LENGTH,), name=\"attention_mask\", dtype=tf.int32)\n","\n","    longformer_output = longformer([input_ids, attention_mask])\n","    net = longformer_output['pooler_output']\n","    \n","    net = tf.keras.layers.Dropout(0.1)(net)\n","    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n","    \n","    output = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"sigmoid\")(net)\n","    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=[net])\n","    \n","    return model\n","\n","# version = \"pooler_test\"\n","# model = create_pooler_model()\n","# model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UacwcUIE220c"},"outputs":[],"source":["# Experiment 1 - CLS\n","\n","def create_cls_model():\n","    input_ids = tf.keras.layers.Input((MODEL_MAX_LENGTH,), name=\"input_ids\", dtype=tf.int32)\n","    attention_mask = tf.keras.layers.Input((MODEL_MAX_LENGTH,), name=\"attention_mask\", dtype=tf.int32)\n","    \n","    longformer_output = longformer([input_ids, attention_mask])\n","    cls_output = longformer_output[\"last_hidden_state\"][:,0,:]\n","    \n","    hidden = tf.keras.layers.Dense(32, activation=\"tanh\", name=\"tanh\")(cls_output)\n","    output = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"sigmoid\")(hidden)\n","    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=[output])\n","\n","    return model\n","\n","version = \"cls\"\n","model = create_cls_model()\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2R9I3vrtHpw6"},"outputs":[],"source":["# Experiment 2 - word embeddings average.\n","\n","def create_avg_model():\n","    input_ids = tf.keras.layers.Input((MODEL_MAX_LENGTH,), name=\"input_ids\", dtype=tf.int32)\n","    attention_mask = tf.keras.layers.Input((MODEL_MAX_LENGTH,), name=\"attention_mask\", dtype=tf.int32)\n","    \n","    longformer_output = longformer([input_ids, attention_mask])\n","    avg_output = tf.keras.layers.GlobalAveragePooling1D()(longformer_output.last_hidden_state, mask=attention_mask)\n","    hidden = tf.keras.layers.Dense(32, activation=\"tanh\")(avg_output)\n","    output = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"sigmoid\")(hidden)\n","    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=[output])\n","\n","    return model\n","\n","# version = \"avg_embeddings\"\n","# model = create_avg_model()\n","# model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XL7h0bHgGtA3"},"outputs":[],"source":["# Experiment 3 - BERT + CNN - ??????\n","\n","# token_ids = tf.keras.layers.Input((None,), dtype=np.int32, name=\"input_ids\")\n","# attention = tf.keras.layers.Input((None,), dtype=np.int32, name=\"attention_mask\")\n","# longformer_output = longformer(input_ids=token_ids, attention_mask=attention)\n","# net = longformer_outputs[\"last_hidden_state\"] # [batch_size, seq_length, 768]\n","# net = tf.keras.layers.Conv1D(32, (2), activation='relu')(net)\n","# net = tf.keras.layers.Conv1D(64, (2), activation='relu')(net)\n","# net = tf.keras.layers.GlobalMaxPool1D()(net)\n","# net = tf.keras.layers.Dense(4096, activation=\"relu\")(net)\n","# net = tf.keras.layers.Dropout(0.1)(net)\n","# net = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"sigmoid\")(net)\n","# model = tf.keras.Model(inputs=[token_ids, attention], outputs=[net])\n","\n","# model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7WjsPn-Rq_aS"},"outputs":[],"source":["tf.keras.utils.plot_model(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UB8uYqqYIh8M"},"outputs":[],"source":["checkpoint_path = os.path.join(DRIVE, f\"checkpoints/tf_longformer_{version}\")\n","\n","# Define callbacks\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_path, \n","    verbose=1, \n","    save_weights_only=True,\n","    save_best_only=True,\n","    save_freq='epoch'\n",")\n","\n","# Use this callback to stop after one epoch if the loss does not improve.\n","early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2UOV7Ld4z-nm"},"outputs":[],"source":["# Configure the loss function, metrics and optimizer\n","loss = tf.keras.losses.BinaryCrossentropy()\n","\n","metrics = [\n","    tf.metrics.BinaryAccuracy(),\n","    tf.keras.metrics.BinaryCrossentropy(),\n","    tf.keras.metrics.Precision(),\n","    tf.keras.metrics.Recall(),\n","    tf.keras.metrics.AUC(), \n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tyuNJZWcJ45C"},"outputs":[],"source":["# Check if layer 2 is indeed the Longformer layer.\n","print(model.layers[2])\n","\n","# Freeze the Longformer layer and compile the model.\n","model.layers[2].trainable = False\n","\n","epochs = 100\n","steps_per_epoch = len(train_labels) # tf.data.experimental.UNKNOWN_CARDINALITY\n","num_train_steps = steps_per_epoch * epochs\n","num_warmup_steps = int(0.1*num_train_steps)\n","\n","# epochs = 1\n","# steps_per_epoch = 1\n","# num_warmup_steps = 0\n","# num_train_steps = 1\n","\n","init_lr = 3e-5\n","optimizer = optimization.create_optimizer(init_lr=init_lr,\n","                                          num_train_steps=num_train_steps,\n","                                          num_warmup_steps=num_warmup_steps,\n","                                          optimizer_type='adamw')\n","\n","model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"igiiP-dqJL8S"},"outputs":[],"source":["# Train the model with freezed longformer layer.\n","history = model.fit(\n","    x=[train_tokens, train_attention],\n","    y=train_labels,\n","    batch_size=1,\n","    validation_data = ([val_tokens, val_attention], val_labels),\n","    validation_batch_size=1,\n","    epochs=epochs,\n","    class_weight=class_weight,\n","    steps_per_epoch=steps_per_epoch,\n","    callbacks=[\n","        early_stopping_callback,\n","        WandbCallback(),\n","        cp_callback,\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lAM5fKYVJUs5"},"outputs":[],"source":["# pprint(history.history)\n","# plot_validation_curves(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HLT6j0zK1-mH"},"outputs":[],"source":["# Check if layer 2 is indeed the Longformer layer.\n","print(model.layers[2])\n","\n","# Unfreeze the Longformer layer and recompile the model.\n","model.layers[2].trainable = True\n","print(model.summary())\n","\n","epochs = 2\n","init_lr = 1e-6\n","optimizer = optimization.create_optimizer(init_lr=init_lr,\n","                                          num_train_steps=num_train_steps,\n","                                          num_warmup_steps=num_warmup_steps,\n","                                          optimizer_type='adamw')\n","\n","model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"]},{"cell_type":"code","source":["# Fit the model with unfreezed longformer layer.\n","history_2 = model.fit(\n","    x=[train_tokens, train_attention],\n","    y=train_labels,\n","    batch_size=1,\n","    validation_data = ([val_tokens, val_attention], val_labels),\n","    validation_batch_size=1,\n","    epochs=epochs,\n","    class_weight=class_weight,\n","    steps_per_epoch=steps_per_epoch,\n","    callbacks=[\n","        WandbCallback(),\n","        cp_callback,\n","    ]\n",")"],"metadata":{"id":"n3-MAbvBfP0N"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PAE_cmiItDlB"},"outputs":[],"source":["pprint(history_2.history)\n","plot_validation_curves(history_2)"]},{"cell_type":"code","source":["# import gc\n","# gc.collect()"],"metadata":{"id":"DcYDfph2j7LL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the weights of the trained model.\n","save_path = os.path.join(DRIVE, f\"tf_longformer_{version}\")\n","\n","# Save the weights\n","model.save_weights(save_path)"],"metadata":{"id":"Iz-xSsw6UUij"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a new model instance.\n","new_model = create_cls_model()\n","\n","# Assign the optimizer and compile.\n","new_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","\n","# Restore the weights.\n","new_model.load_weights(save_path)"],"metadata":{"id":"7HpayZaFffcv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIlxc7QCu4Lm"},"outputs":[],"source":["eval = new_model.evaluate(x=test_inputs, y=test_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EhrCWazuoeYX"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Copy of Longformer Keras.ipynb","provenance":[{"file_id":"1e9_ShLBqRQkMim1Si4pUOycgLXmAwC0b","timestamp":1655385399221}],"authorship_tag":"ABX9TyPDRJc7en87DqB1IU6K48PB"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}