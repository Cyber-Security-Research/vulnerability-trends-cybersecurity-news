import json
import numpy as np
import pandas as pd

from typing import List, Tuple
from itertools import chain

from datetime import datetime
from pprint import pprint

from collections import Counter, OrderedDict
from transformers import TFAutoModel, AutoTokenizer


def read_json_file(filename: str) -> dict:
    with open(filename, 'r') as f:
        return json.load(f)


def extract_tokens_counts(dataset: List[dict]):
    tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096')
    for document in dataset:
        tokenized_input = tokenizer(document['title'] + ' ' + document['text'])
        yield len(tokenized_input['input_ids'])


def compose_intervals() -> List[Tuple[int, int]]:
    """
    Ox axis.
    Steps: 64, 128 and 256.
    """

    values = list(chain(
        range(0, 1024, 64),
        range(1024, 2048, 128),
        range(2048, 4097, 256)
    ))

    intervals = list(zip(values[:-1], values[1:]))
    def decrease_one_for_interval(intervals):
        for (start, end) in intervals:
            yield (start, end - 1)

    return list(decrease_one_for_interval(intervals[:-1])) + [intervals[-1]]


def count_tokens_length_per_interval(intervals: List[Tuple[int, int]]):
    """
    Create dictionary with intervals and documents' counts.
    """

    results = OrderedDict()
    for (start, end) in intervals:
        for length in lengths_list:
            if start <= length < end:
                name = str(start) + '-\n' + str(end)
                results.setdefault(name, 0)
                results[name] += 1
    return results



if __name__ == '__main__':

    filename = 'balanced_cyber_security_news_dataset.json'

    dataset = read_json_file(filename)

    tokens_count_list = [item for item in extract_tokens_counts(dataset)]
    tokens_count_list.sort()

    results = count_tokens_length_per_interval(intervals)

    pprint(results)

